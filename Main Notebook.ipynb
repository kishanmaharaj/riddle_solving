{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c372e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65083b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "\n",
    "my_token = \"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\"\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"danlou/albert-xxlarge-v2-finetuned-csqa\", use_auth_token = my_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57144d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"danlou/albert-xxlarge-v2-finetuned-csqa\", use_auth_token = my_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "167da8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "# choice0 = \"It is eaten with a fork and a knife.\"\n",
    "# choice1 = \"It is eaten while held in the hand.\"\n",
    "# labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "# encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n",
    "# outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "# torch.softmax(outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af39f8f3",
   "metadata": {},
   "source": [
    "## Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a875a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='std_albert_script.log', filemode='w', format='%(asctime)s : %(name)s : %(levelname)s : %(message)s')\n",
    "logger=logging.getLogger() \n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5c364",
   "metadata": {},
   "source": [
    "## Import Riddle Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb55e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "riddle_sense = []\n",
    "with jsonlines.open('riddle_sense/rs_train.jsonl') as f:\n",
    "    for line in f:\n",
    "        riddle_sense.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe3f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41994c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_form_opt =  { \"A\": 0, \"B\": 1, \"C\":2, \"D\":3, \"E\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda9318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# riddle_sense[0][ 'answerKey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32392120",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(\"Current Device: \" + device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c09eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09bf1c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inference = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e1f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(riddle):\n",
    "    \n",
    "    choices = riddle['choices']    \n",
    "    \n",
    "    # create k(number of options)  duplicates of the question \n",
    "    option_count = len(choices)\n",
    "    questions = ((riddle['question'] + \"~~\") * option_count).split(\"~~\")[:-1]\n",
    "\n",
    "\n",
    "    encoding = tokenizer(questions, choices, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})\n",
    "    probability = torch.softmax(outputs.logits, dim=1) \n",
    "    prediction = torch.argmax(probability, dim=1).item()\n",
    "    probability = probability.tolist()[0]\n",
    "\n",
    "    proba_dist = \" \"\n",
    "    \n",
    "    for i in range(len(probability)):\n",
    "        proba_dist = proba_dist + choices[i]+ \":\"\n",
    "        proba_dist = proba_dist + str(round(probability[i], 4))\n",
    "        proba_dist = proba_dist + \" -> \"\n",
    "        \n",
    "    \n",
    "    return \"Prediction : \"+ str(prediction) + \"  || \" +\"Probability Distribution --> \" + proba_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5593ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# riddle = {\n",
    "#     'question': 'A man is incarcerated in prison, and as his punishment he has to carry a one tonne bag of sand backwards and forwards across a field the size of a football pitch.  What is the one thing he can put in it to make it lighter?'\n",
    "# , 'choices': ['throw', 'bit', 'gallon', 'mouse', 'hole']}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75249b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference(riddle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0963b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f391393",
   "metadata": {},
   "source": [
    "### Finetuned on CSQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6933c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3510/3510 [02:52<00:00, 20.37it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "answer_key = []\n",
    "\n",
    "corrected_count = 0\n",
    "\n",
    "\n",
    "for riddle_num in tqdm(range(len(riddle_sense))):\n",
    "    questions = []\n",
    "    choices = []\n",
    "\n",
    "    for i in range(0,5):\n",
    "        questions.append(riddle_sense[riddle_num]['question']['stem'])\n",
    "        choices.append(riddle_sense[riddle_num]['question']['choices'][i][\"text\"])\n",
    "\n",
    "    encoding = tokenizer(questions, choices, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})\n",
    "    probability = torch.softmax(outputs.logits, dim=1)\n",
    "    prediction = options[torch.argmax(probability, dim=1).item()]\n",
    "    current_answer = riddle_sense[riddle_num][ 'answerKey']\n",
    "    \n",
    "    if not prediction == current_answer:\n",
    "        pred_index = numbers_form_opt[prediction]\n",
    "        answer_index = numbers_form_opt[current_answer]\n",
    "\n",
    "        \n",
    "        masked_choices = choices.copy()\n",
    "        masked_choices[pred_index] = \"XXXXXXX\"\n",
    "        \n",
    "        ablated_answer = choices[answer_index]\n",
    "        \n",
    "\n",
    "        \n",
    "        encoding = tokenizer(questions, masked_choices, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs_ablated = model(**{k: v.unsqueeze(0) for k, v in encoding.items()})\n",
    "        probability_ablated = torch.softmax(outputs_ablated.logits, dim=1)\n",
    "        prediction_ablated = masked_choices[torch.argmax(probability_ablated, dim=1).item()]\n",
    "        \n",
    "#         logger.info(\"Trail.  Question: \" + riddle_sense[riddle_num]['question']['stem']+ \" | Choices: \"+ \", \".join(choices)+ \"| Initial Prediction  :\" + choices[numbers_form_opt[prediction]] + \" | Ablated Prediction: \" + prediction_ablated + \" | Answer: \" +ablated_answer)\n",
    "        \n",
    "        if prediction_ablated == ablated_answer:\n",
    "            corrected_count = corrected_count + 1\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    answer_key.append(current_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d06aed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'A', 'text': 'crowd'},\n",
       " {'label': 'B', 'text': 'characteristic'},\n",
       " {'label': 'C', 'text': 'sportsmanlike'},\n",
       " {'label': 'D', 'text': 'all the fans left'},\n",
       " {'label': 'E', 'text': 'shade'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riddle_sense[riddle_num]['question']['choices'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf93403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d018794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2ef223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf62b72",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caafa9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5262108262108263"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initial Accuracy\n",
    "initial_accuracy = ((sum(np.array(predictions[:num])==np.array(answer_key[:num])))/len(predictions[:num]))\n",
    "initial_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "675da90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6518518518518519"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_accuracy = ((sum(np.array(predictions[:num])==np.array(answer_key[:num]))+corrected_count)/len(predictions[:num]))\n",
    "mask_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "864348b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12564102564102564"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_accuracy-initial_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53702cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"id\": \"xzc81234ioioucv-1052\",\n",
    " \"question\": {\"stem\": \"What comes in many different sizes but is always only 1 foot long?\",\n",
    "              \"choices\": [{\"label\": \"A\", \"text\": \"object\"}, {\"label\": \"B\", \"text\": \"shoe\"}, {\"label\": \"C\", \"text\": \"footing\"},\n",
    "                          {\"label\": \"D\", \"text\": \"grave\"}, {\"label\": \"E\", \"text\": \"column\"}]}, \"answerKey\": \"B\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd08573",
   "metadata": {},
   "source": [
    "### Finetuned on riddlesense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f8a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d37523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "debc65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d09efdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9186ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "csqa_dataset = load_dataset(\"commonsense_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5be159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 9741\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 1221\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'question_concept', 'choices', 'answerKey'],\n",
       "        num_rows: 1140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csqa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07bd18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from datasets import load_dataset\n",
    "riddle_sense_dataset = load_dataset('json',  data_files={'train': ['riddle_sense/rs_train.jsonl'], 'validation': ['riddle_sense/rs_dev.jsonl']} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b29248d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'A', 'text': 'throw'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riddle_sense_dataset['train'][0]['question']['choices'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac6b64fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cxzvpoiuzckf-3123',\n",
       " 'question': {'stem': 'A man is incarcerated in prison, and as his punishment he has to carry a one tonne bag of sand backwards and forwards across a field the size of a football pitch.  What is the one thing he can put in it to make it lighter?',\n",
       "  'choices': [{'label': 'A', 'text': 'throw'},\n",
       "   {'label': 'B', 'text': 'bit'},\n",
       "   {'label': 'C', 'text': 'gallon'},\n",
       "   {'label': 'D', 'text': 'mouse'},\n",
       "   {'label': 'E', 'text': 'hole'}]},\n",
       " 'answerKey': 'E'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riddle_sense[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87bb09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "\n",
    "    questions = []\n",
    "    choices = []\n",
    "\n",
    "    for i in range(0,5):\n",
    "#         questions.append(riddle_sense_dataset['train'][0]['question']['stem'])\n",
    "#         choices.append(riddle_sense_dataset['train'][0]['question']['choices'][i][\"text\"])\n",
    "        questions.append(examples['question']['stem'])\n",
    "        choices.append(examples['question']['choices'][i][\"text\"])\n",
    "\n",
    "\n",
    "    encoding = tokenizer(questions, choices, return_tensors=\"pt\",  truncation=True, padding=True).to(device)\n",
    "    return {k: v.unsqueeze(0) for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f34035f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013463973999023438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3510,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66261460cd0242f78878ff6fdc74d8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3510 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019645214080810547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1021,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe340a051f54824b754f191128c0937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = riddle_sense_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d67d461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answerKey', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3510\n",
       "})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d43a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"question\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_datasets = riddle_sense_dataset.map(tokenizer, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ce9dddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encodings = tokenizer(riddle_sense_dataset['train'], truncation=True, padding=True)\n",
    "# val_encodings = tokenizer(riddle_sense_dataset['validation'], truncation=True, padding=True)\n",
    "# test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70a75fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        print(features[0].keys())\n",
    "        label_name = \"answerKey\" if \"answerKey\" in features[0].keys() else \"answerKey\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b5b8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'question', 'answerKey', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63618f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "307d599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answerKey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1367\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1364\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1366\u001b[0m )\n\u001b[0;32m-> 1367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1583\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1582\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1583\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1584\u001b[0m \n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1587\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36mDataCollatorForMultipleChoice.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     20\u001b[0m label_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature\u001b[38;5;241m.\u001b[39mpop(label_name) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     22\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features)\n\u001b[1;32m     23\u001b[0m num_choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     20\u001b[0m label_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswerKey\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m     22\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features)\n\u001b[1;32m     23\u001b[0m num_choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answerKey'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90ee3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "swag = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12ddd80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7cffc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenized_swag = swag.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0b0be98a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_swag_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_swag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_swag\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataCollatorForMultipleChoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:464\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Create clone of distant repo and output directory if needed\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_git_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# In case of pull, we need to make sure every process has the latest.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:3058\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3056\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m repo_name:\n\u001b[0;32m-> 3058\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[43mget_full_repo_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3061\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo \u001b[38;5;241m=\u001b[39m Repository(\n\u001b[1;32m   3062\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[1;32m   3063\u001b[0m         clone_from\u001b[38;5;241m=\u001b[39mrepo_name,\n\u001b[1;32m   3064\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   3065\u001b[0m         private\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_private_repo,\n\u001b[1;32m   3066\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:1028\u001b[0m, in \u001b[0;36mget_full_repo_name\u001b[0;34m(model_id, organization, token)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     token \u001b[38;5;241m=\u001b[39m HfFolder\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1028\u001b[0m     username \u001b[38;5;241m=\u001b[39m \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py:843\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhoami\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    Call HF API to know \"whoami\".\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m            not provided.\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/whoami-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 843\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# If `token` is provided and not `None`, it will be used by default.\u001b[39;49;00m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Otherwise, the token must be retrieved from cache or env variable.\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    848\u001b[0m     )\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py:4732\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   4729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4730\u001b[0m     \u001b[38;5;66;03m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   4731\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken\n\u001b[0;32m-> 4732\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_hf_headers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_write_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_write_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4737\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    118\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_headers.py:117\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mBuild headers dictionary to send in a HF Hub call.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m        If `token=True` but token is not saved locally.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Get auth token to send\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m token_to_send \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_to_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[38;5;241m=\u001b[39mis_write_action)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Combine headers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_headers.py:149\u001b[0m, in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is required (`token=True`), but no token found. You\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m need to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/settings/tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_token\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Case implicit use of the token is forbidden by env variable\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_swag_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_swag[\"train\"],\n",
    "    eval_dataset=tokenized_swag[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f022dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
